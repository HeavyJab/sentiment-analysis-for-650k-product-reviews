---
title: "Assignment 2 - Sentiment Analysis"
author: "Vincent Lam, Cindy Inanto"
date: "01/05/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries
library(tidyverse)
library(quanteda)
library(caret)
library(xgboost)
library(randomForest)
library(glmnet)
library(parallel)
library(parallelMap) 
library(text2vec)
require(doMC)

# importing the data
train <- read_csv("~/Google Drive/Monash/Semester 3/FIT5149/Assessment/Assessment 2/src/Training Dataset/train_data.csv")
label <- read_csv("~/Google Drive/Monash/Semester 3/FIT5149/Assessment/Assessment 2/src/Training Dataset/train_label.csv")

train <- merge(x=train, y=label, by='trn_id')

# set the memory limit to whatever is possible
memory.limit(size = NA)

# quanteda options settings
quanteda_options(verbose = TRUE)
quanteda_options(threads = 8)

# use all cores
parallelStartSocket(cpus = detectCores())
```

# Base Models
```{r Train Test Split}
# use a smaller subset to perform analysis
train <- train[1:6500,]

set.seed(162)

# size of the sample 
sample <- floor(0.7*nrow(train))

train_dfm <- train %>% 
  corpus() %>% 
  dfm(remove_punct=TRUE, 
      removeNumbers = TRUE,
      remove=stopwords("english")) %>% 
  dfm_sample(size=sample,
             margin='documents') 

test_dfm <- train %>% 
  corpus() %>% 
  corpus_subset(!docnames(.) %in% docnames(train_dfm)) %>% 
  dfm(remove_punct=TRUE, 
      remove=stopwords("english")) %>% 
  # make sure the dfm has all the features train dfm has
  dfm_select(pattern=train_dfm,
             selection='keep')

train_slim <- dfm_trim(train_dfm,
                     sparsity=0.95,
                     min_docfreq = 0.05,
                     docfreq_type = "prop",
                     verbose = TRUE)

# test set 
test_slim <- dfm_match(test_dfm, 
                  features = featnames(train_slim))

# accuracy function for evaluation
accuracy <- function(ypred, y){
    tab <- table(ypred, y)
    return(sum(diag(tab))/sum(tab))
}
```

```{r Accurary Evaluation}
# accuracy function to evaluate predictions

```

## Naive Baysian Model
Quanteda library comes with a naive baysian classifier and we will quickly set a benchmark for the task

https://tutorials.quanteda.io/machine-learning/nb/

```{r Naive Baysian With Full Set of Tokens}
# the model and prediction with full feature set

nb_model <- function(train, test){
  print(train)
  nb <- textmodel_nb(train, 
                        docvars(train, 'label'), 
                        prior='docfreq')

  pred <- predict(nb, newdata=test)

  # scoring
  accuracy(docvars(test, 'label'), pred)
}

nb_model(train_dfm, test_dfm)
```


```{r Naive Baysian With Trimed DFM}
train_dfm

# has to appear in at least 0.5 percent of the documents

nb_model(train_slim, test_slim)
```

## Support Vector Machine
```{r SVM with parameter tuning}
library(e1071)

svm_model <- function(train, test){
  svm_model <- tune(svm, train.x=train, 
            train.y=factor(docvars(train, 'label')),
            type='C-classification',
            kernel='radial',
            ranges=list(cost=c(0.1, 1, 5, 10)))

  pred <- predict(svm_model$best.model, newdata=test)

  accuracy(docvars(test, 'label'), pred)
}

svm_model(train_slim, test_slim)
```

## Logistic Regression
```{r Logistic Regression}

log_model <- function(train, test, fold){
  registerDoMC(cores=3)
  ridge <- cv.glmnet(train, factor(docvars(train, 'label')), 
      family="multinomial", alpha=1, nfolds=fold, parallel=TRUE, intercept=TRUE,
      type.measure="class")
  
  pred <- predict(ridge, test, type="class")
  
  accuracy(docvars(test, 'label'), pred)
}

log_model(train_slim, test_slim, 30)
```

```{r}
?registerDoMC()
```

## Random Forrest
```{r Random Forrest}
library(randomForest)

rf_model <- function(train, test, try, ntree){
  df_train <- convert(train, to='data.frame')[,-1]
  df_test <- convert(test, to='data.frame')[,-1]

  rf <- randomForest(x=df_train, 
                     y=factor(docvars(train, 'label')),
                     xtest=df_test,
                     ytest=factor(docvars(test, 'label')),
                     importance=TRUE,
                     mtry=try,
                     ntree=ntree,
                     keep.forest=TRUE
                     )

  pred <- predict(rf, df_test, type="response")
  accuracy(docvars(test, 'label'), pred)
}

rf_model(train_slim, test_slim, 20, 100)
```

## XGBoost Random Forrest
```{r}

xgb_model <- function(train, test, round, eta, dep){
  # change to dataframe
  df_train <- convert(train, to='data.frame')[,-1]
  df_test <- convert(test, to='data.frame')[,-1]
  
  # info about the documents
  print(train)
  
  # model
  xgb <- xgboost(data = as.matrix(df_train), 
  # label must be in [0, num_class) rather than 1 - 5
  label =  (as.numeric(docvars(train, 'label'))-1), 
  eta = eta,
  max_depth = dep, 
  nround=round, 
  gamma=5,
  subsample = 0.6,
  booster = 'gbtree',
  early_stopping_round = 20,
  colsample_bytree = 0.7,
  seed = 1,
  eval_metric = "merror",
  objective = "multi:softmax",
  num_class = 5,
  )
  
  pred <- predict(xgb, test) + 1
  accuracy(docvars(test, 'label'), pred)
}

xgb_model(train_slim, test_slim, 5, 0.2, 10)
```

# Feature Selection & Extraction
## TF-IDF Normalization
It won't be a big improvement as shown in [2].

```{r Tf-idf Normalization}
train_tfidf <- train_slim %>% 
  dfm_tfidf(scheme_tf = "count", 
            scheme_df = "inverse", 
            base = 10,
            force = FALSE)

test_tfidf <- dfm_match(test_dfm, 
                  features = featnames(train_tfidf))

```

## Chi-squared
[5] 
```{r Chi-squared }
chi2 <- dfm_group(train_slim, 
          groups = 'label') %>% 
  textstat_keyness()

chi2$chi2 <- abs(chi2$chi2)

chi2 <- chi2[order(chi2$chi2, decreasing = TRUE), ]
head(chi2,10)

train_chi <- train_dfm %>% 
  dfm_select(chi2$feature)

test_chi <- dfm_match(test_dfm, 
                  features = featnames(train_chi))

xgb_model(train_slim, test_slim, 5, 0.2, 10)
xgb_model(train_chi, test_chi, 5, 0.2, 10)
xgb_model(train_tfidf, test_tfidf, 5, 0.2, 10)

rf_model(train_slim, test_slim, 20, 50)
rf_model(train_chi, test_chi, 20, 50)
rf_model(train_tfidf, test_tfidf, 20, 50)

nb_model(train_slim, test_slim)
nb_model(train_chi, test_chi)

log_model(train_slim, test_slim, 30)
log_model(train_chi, test_chi, 30)
log_model(train_tfidf, test_tfidf, 30)
```


## Ngrams Features
```{r Bigrams}

train_ngrams <- train %>% 
  dfm(stem=TRUE,
      remove=stopwords("english"), 
      ngrams=1:3, 
      verbose=TRUE)

```

## Specific Words Features
```{r EDA on labels}
library(mldr)

```

```{r Negation}
tokens_compound()
```

```{r Key words}
# create tokens from iterator
vocab <- it_train %>% 
  create_vocabulary(stopwords=stopwords('english'), 
                    ngram = c(ngram_min = 1L,
                               ngram_max = 3L)) %>% 
  prune_vocabulary(term_count_min = 10, 
                   doc_proportion_max = 0.5,
                   doc_proportion_min = 0.001)

# vectorize the vocab
vect <- vocab_vectorizer(vocab)
```

# God Speed
```{r Text2Vec}
# train test
set.seed(100)

smp_size <- floor(0.75 * nrow(train))

split<-sample(seq_len(nrow(train)), size = smp_size)
train_df <- train[split,]
test_df <- train_df[-split,]

# preprocessing functions
prep_fun = tolower
tok_fun = word_tokenizer

# iterator to apply functions on token
it_train <- itoken(train_df$text, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = train_df$trn_id, 
             progressbar = TRUE)

it_test <- itoken(test_df$text, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = test_df$trn_id, 
             progressbar = TRUE)

hash <- hash_vectorizer(hash_size = 2 ^ 16, 
                        ngram = c(ngram_min = 1L,
                                  ngram_max = 2L))
# tfidf
tfidf <- TfIdf$new()

# create dfm using ngram vectorizer
dtm_train <- it_train %>% 
  create_dtm(hash) %>% 
  normalize("l1") %>% 
  fit_transform(tfidf) 
  
dtm_test = it_test %>% 
  create_dtm(hash) %>% 
  normalize("l1") %>% 
  transform(tfidf)

dim(dtm_train)
dim(dtm_test)
```
```{r God Speed}
registerDoMC(cores=detectCores())
ridge <- cv.glmnet(dtm_train, factor(train_df$label), 
      family="multinomial", alpha=1, nfolds=5, parallel=TRUE, intercept=TRUE,
      type.measure="class")
  
pred <- predict(ridge, dtm_test, type="class")
  
accuracy(test_df$label, pred)
```


## Sentiment Scores 
```{r Sentiment Feature}

# AFINN sentiment -5 - 5
sentiment <- get_sentiments("afinn")

# join the sentiment words with token
tokens <- tokens %>% 
  left_join(sentiment)
```

# Model Tuning
[3] states the optimal cv folds

[4] How to tune parameter
```{r Parameter tuning}

```


### Reference
https://www.tidytextmining.com/tidytext.html
2. https://appliedmachinelearning.blog/2017/02/12/sentiment-analysis-using-tf-idf-weighting-pythonscikit-learn/
3. http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm
4. https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/
5. https://stackoverflow.com/questions/38538821/feature-selection-in-document-feature-matrix-by-using-chi-squared-test


